{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# from src.run_all.main_preprocess import load_data, add_features\n",
    "from src.utilities.utilities import get_latest_file\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Load original sources and combine to one DataFrame\n",
    "# df_dataset_WMO = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Feature engineering to get more features\n",
    "# df_dataset_WMO_with_features = add_features(df_dataset_WMO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Write temporary result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suffix_datetime = datetime.strftime(datetime.now(), format='%Y%m%d%H%M')\n",
    "\n",
    "# df_dataset_WMO_with_features.to_parquet(f'../../data/df_preprocess_WMO_{suffix_datetime}.parquet.gzip',\n",
    "#               compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Load previous dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Continue with loaded data from preprocess\n",
    "# df = df_dataset_WMO_with_features.copy()\n",
    "\n",
    "# ## HARDCODED\n",
    "# datapath = '../../data/'\n",
    "# filename = 'df_preprocess_WMO_202103211137.parquet.gzip'\n",
    "# df = pd.read_parquet(datapath + filename)\n",
    "\n",
    "# ## SELECT LAST FILE\n",
    "datapath = '../data/'\n",
    "df = get_latest_file(filename_str_contains='df_get_data_WMO_WIJK_HUISHOUDENS_BEVOLKING_HEFFING_202104241837', datapath=datapath, filetype='parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zorgen voor de juiste modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RepeatedKFold, GridSearchCV, cross_validate, KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, LassoCV, ElasticNet, BayesianRidge\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from xgboost import XGBRegressor, XGBClassifier, plot_importance\n",
    "\n",
    "from src.utilities.transformers import ColumnSelector\n",
    "\n",
    "# instellingen voor panda weergave aanpassen\n",
    "pd.set_option('display.max_rows', 500) # alle rijen tonen\n",
    "pd.set_option('display.max_columns', 500) # alle kolommen tonen\n",
    "pd.set_option('display.width', 1000) # kolombreedte\n",
    "pd.set_option(\"display.precision\", 2)     # precisie van de kolommen aanpassen\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) # floats output tot 3 decimalen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataframe parameters\n",
    "# locatie van dataset \n",
    "DF_LOCATION = 'C:/_NoBackup/Git/__JADS/WMO_execute_group_project/data/df_dataset_WMO.parquet.gzip'\n",
    "# Location all data\n",
    "datapath = '../../data/'\n",
    "# manier van laden dataset. Bijvoorbeeld read_parquet of read_csv\n",
    "DF_READ = pd.read_parquet\n",
    "\n",
    "## X & Y parameters\n",
    "# de kolommen die uit de X dataset moeten worden gehaald. Dat is in ieder geval de y en eventueel nog meer kolommen.\n",
    "# X_DROP_VALUES = ['wmoclienten', 'eenpersoonshuishoudens', 'huishoudenszonderkinderen', 'huishoudensmetkinderen']\n",
    "X_DROP_VALUES = ['wmoclienten', 'wmoclientenper1000inwoners', 'bedrijfsmotorvoertuigen',\n",
    "                 'perioden', 'popcodea', 'popcodeb', 'popcodec', 'popcoded', 'popcodee', 'popcodef', 'popcodeg', 'popcodeh', \n",
    "                'popcodei', 'popcodej', 'popcodek', 'popcodel', 'popcodem', 'popcoden', 'popcodeo', 'popcodep', 'popcodeq', \n",
    "                'popcoder', 'popnaama', 'popnaamb', 'popnaamc', 'popnaamd', 'popnaame', 'popnaamf', 'popnaamg', \n",
    "                'popnaamh', 'popnaami', 'popnaamj', 'popnaamk', 'popnaaml', 'popnaamm', 'popnaamn', 'popnaamo',\n",
    "                'popnaamp', 'popnaamq', 'popnaamr', 'popkoppelvariabeleregiocode', 'typemaatwerkarrangement', \n",
    "                'gemeentenaam', 'meestvoorkomendepostcode', 'dekkingspercentage', \n",
    "                 'gemgestandaardiseerdinkomenvanhuish', 'huishoudenstot110vansociaalminimum', \n",
    "                 'huishoudenstot120vansociaalminimum', 'mediaanvermogenvanparticulierehuish', \n",
    "                 'popafstandtotopenbaargroen', 'popafstandtotsportterrein', 'popagrarischterreinopp', \n",
    "                 'popagrarischterreinperc', 'popagrarischterreinperinwoner', 'popbebouwdterreinopp', \n",
    "                 'popbebouwdterreinperc', 'popbebouwdterreinperinwoner', 'popbosenopennatuurlijkterreinopp', \n",
    "                 'popbosenopennatuurlijkterreinperc', 'popbosenopennatuurlijkterreinperinwoner', 'popgemeenten', \n",
    "                 'poprecreatieterreinopp', 'poprecreatieterreinperc', 'poprecreatieterreinperinwoner', \n",
    "                 'popsemibebouwdterreinopp', 'popsemibebouwdterreinperc', 'popsemibebouwdterreinperinwoner', \n",
    "                 'popverkeersterreinopp', 'popverkeersterreinperc', 'popverkeersterreinperinwoner']\n",
    "# de kolom die wordt gebruikt als y value\n",
    "Y_VALUE = ['wmoclientenper1000inwoners']\n",
    "# test size voor de train/test split\n",
    "TEST_SIZE = 0.3\n",
    "# random state voor de train/test split. Bijvoorbeeld random_state = 42 als vaste seed voor reproduceerbaarheid\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "## Pipeline parameters\n",
    "# strategy en waarde om te vullen bij lege categorische kolommen\n",
    "NAN_VALUES_CAT_STRATEGY = 'constant'\n",
    "NAN_VALUES_CAT_VALUES = 'Missing'\n",
    "# waarden om in te vullen bij lege numerieke kolommen. Bijvoorbeeld mean of median\n",
    "NAN_VALUES_NUM_STRATEGY = 'mean'\n",
    "# \n",
    "#COLS_SELECT = ['aantalinwoners', 'mannen', 'vrouwen', 'k0tot15jaar'\n",
    "#               , 'k15tot25jaar', 'k25tot45jaar', 'k45tot65jaar', 'k65jaarofouder', 'gescheiden'\n",
    "#               , 'verweduwd', 'westerstotaal', 'sterftetotaal', 'gemiddeldehuishoudensgrootte'\n",
    "#               , 'gemiddeldewoningwaarde', 'koopwoningen', 'huurwoningentotaal', 'inbezitwoningcorporatie'\n",
    "#               , 'gemiddeldinkomenperinkomensontvanger', 'k40personenmetlaagsteinkomen', 'k20personenmethoogsteinkomen'\n",
    "#               , 'actieven1575jaar', 'k40huishoudensmetlaagsteinkomen', 'k20huishoudensmethoogsteinkomen'\n",
    "#               , 'huishoudensmeteenlaaginkomen', 'personenpersoortuitkeringaow', 'rucultuurrecreatieoverigediensten'\n",
    "#               , 'personenautosperhuishouden', 'matevanstedelijkheid']\n",
    "COLS_SELECT = None\n",
    "\n",
    "## Model parameters\n",
    "# manier van cross validate in de modellen. Bijvoorbeeld 10 of RepeatedKFold(n_splits=30, n_repeats=5, random_state=1)\n",
    "CROSS_VALIDATE = 10\n",
    "# manier van scoren in de modellen\n",
    "MODEL_SCORING = 'neg_mean_squared_error'\n",
    "\n",
    "## Scoring parameters\n",
    "# Deze kunnen we later toevoegen als we meerdere manieren van scoren hebben. Dus niet alleen maar de RSMLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functie maken om op basis van de cv scores, het beste RMLSE model te selecteren \n",
    "def get_best_model_rmsle(cv_scores):\n",
    "    \"\"\"\n",
    "    Return best (most conservative) model from cross_validate object.\n",
    "    \n",
    "    Uses np.argmax to find bottomright point == largest RMSE\n",
    "    \"\"\"\n",
    "    index = np.argmax(np.sqrt(-cv_scores['train_neg_mean_squared_error']))\n",
    "    model = cv_scores['estimator'][index]\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Done before start of 'Train' chapter\n",
    "# df = get_latest_file(mypath=datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stappen hieronder mogelijk verplaatsten naar prepare stap, later beoordelen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droppen van de rijen waar de y_value leeg is, anders kunnen de modellen er niet mee overweg\n",
    "df.dropna(\n",
    "    axis=0,\n",
    "    how='any',\n",
    "    thresh=None,\n",
    "    subset=Y_VALUE,\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X en y aanmaken\n",
    "X = df.drop(X_DROP_VALUES, axis=1)\n",
    "# y = df[Y_VALUE]*100 # 0.01 -> 1.0 percentage\n",
    "y = df[Y_VALUE] # 0.01 -> 1.0 percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitsen van X en y in train/test. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = TEST_SIZE, random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitsen van X_train in categorische en numerieke kolommen, om apart te kunnen transformeren\n",
    "cat_cols = X_train.select_dtypes(include=['category']).columns\n",
    "num_cols = X_train.select_dtypes(include=['int64','float64','float32','int32']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines (pl) maken voor imputing, scaling en OneHotEncoding per datatype \n",
    "\n",
    "# categorie met waarde die is gegeven aan \"MISSING\" toevoegen\n",
    "for col in cat_cols:\n",
    "    # need to add category for missings, otherwise error with OneHotEncoding (volgens mij ook met alleen imputing)\n",
    "    X_train[col].cat.add_categories(NAN_VALUES_CAT_VALUES, inplace=True)\n",
    "categories = [X_train[col].cat.categories for col in cat_cols]\n",
    "\n",
    "# pipeline voor categorial datatype\n",
    "pl_ppc_cat = make_pipeline(\n",
    "     SimpleImputer(\n",
    "         missing_values = np.nan\n",
    "        ,strategy = NAN_VALUES_CAT_STRATEGY\n",
    "        ,fill_value = NAN_VALUES_CAT_VALUES)\n",
    "    ,OneHotEncoder(categories=categories)\n",
    ")\n",
    "\n",
    "# pipeline voor numeriek datatype\n",
    "pl_ppc_num = make_pipeline(\n",
    "      ColumnSelector(cols=COLS_SELECT)\n",
    "    ,SimpleImputer(\n",
    "         missing_values = np.nan\n",
    "        ,strategy = NAN_VALUES_NUM_STRATEGY)\n",
    "    ,StandardScaler()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines maken om de preprocessing van de imputing te combineren\n",
    "pl_ppc_total = make_column_transformer(\n",
    "     (pl_ppc_cat, cat_cols)\n",
    "    ,(pl_ppc_num, num_cols)\n",
    "    ,remainder = 'drop'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance & feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de X train door bovenstaande pipelines heen halen en opslaan in X_train_prepared\n",
    "X_train_prepared = pl_ppc_total.fit_transform(X_train)\n",
    "# de X_train_prepared weer omzetten naar dataframe, inclusief column names\n",
    "X_train = pd.DataFrame(data=X_train_prepared, columns=[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kiezen van het model waarmee feature importance wordt bepaald, bijvoorbeeld:\n",
    "    # RandomForestRegressor(random_state=42)\n",
    "    # XGBRegressor(n_estimators=100, random_state = 42)\n",
    "FI_MODEL = XGBRegressor(n_estimators=100, random_state = 42)\n",
    "FI_MODEL.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = FI_MODEL.feature_importances_\n",
    "fi_list = sorted(zip(fi, num_cols), reverse=True)\n",
    "# met de [:n] kun je het aantal features aanpassen dat getoond moet worden\n",
    "fi_df_top_n = pd.DataFrame(fi_list[:30])\n",
    "fi_df_top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.barh(fi_df_top_n[1], fi_df_top_n[0],)\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso (cross validate) uitvoeren\n",
    "feature_selection = LassoCV(cv=10).fit(X_train, y_train)\n",
    "# lijst aanmaken met coefficienten per feature\n",
    "feature_selection_coef_list = sorted(zip(np.abs(feature_selection.coef_), num_cols), reverse=True)\n",
    "df_coef = pd.DataFrame(feature_selection_coef_list, columns=['coef','feature']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laat de N features zien met de hoogste coefficienten\n",
    "df_coef.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_0_cols_df = df_coef[df_coef[\"coef\"] == 0]\n",
    "coef_0_cols_list = coef_0_cols_df['feature'].values.tolist()\n",
    "coef_0_cols_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do (logistic regression/continious waarden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_model_logistic_regression = LogisticRegression()\n",
    "rfe = RFE(feature_importance_model_logistic_regression, 15)\n",
    "fit = rfe.fit(X_train_prepared, y_train)\n",
    "print(\"Num Features: %s\" % (fit.n_features_))\n",
    "print(\"Selected Features: %s\" % (fit.support_))\n",
    "print(\"Feature Ranking: %s\" % (fit.ranking_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "sel_ = SelectFromModel(LogisticRegression(C=1, penalty='l1'))\n",
    "sel_.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GRIDSEARCH_MODEL = RandomForestRegressor(random_state=42)\n",
    "param_grid = [\n",
    "    # try 12 (3×4) combinations of hyperparameters\n",
    "    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n",
    "    # then try 6 (2×3) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},\n",
    "  ]\n",
    "\n",
    "# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \n",
    "grid_search = GridSearchCV(GRIDSEARCH_MODEL, param_grid, cv=2,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "grid_search.fit(X_train_prepared, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "important_attributes = sorted(zip(feature_importances, num_cols), reverse=True)\n",
    "important_attributes[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:jads-env] *",
   "language": "python",
   "name": "conda-env-jads-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
